[2023-03-11T19:55:39.980+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: s3_to_snowflake.upload_or_snowflake manual__2023-03-11T19:50:13.559472+00:00 [queued]>
[2023-03-11T19:55:39.990+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: s3_to_snowflake.upload_or_snowflake manual__2023-03-11T19:50:13.559472+00:00 [queued]>
[2023-03-11T19:55:39.991+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2023-03-11T19:55:39.992+0000] {taskinstance.py:1280} INFO - Starting attempt 2 of 2
[2023-03-11T19:55:39.994+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2023-03-11T19:55:40.009+0000] {taskinstance.py:1300} INFO - Executing <Task(S3ToSnowflakeOperator): upload_or_snowflake> on 2023-03-11 19:50:13.559472+00:00
[2023-03-11T19:55:40.021+0000] {standard_task_runner.py:55} INFO - Started process 7494 to run task
[2023-03-11T19:55:40.025+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 's3_to_snowflake', 'upload_or_snowflake', 'manual__2023-03-11T19:50:13.559472+00:00', '--job-id', '94', '--raw', '--subdir', 'DAGS_FOLDER/s3_to_snowflake.py', '--cfg-path', '/tmp/tmpgmt29tp5']
[2023-03-11T19:55:40.029+0000] {standard_task_runner.py:83} INFO - Job 94: Subtask upload_or_snowflake
[2023-03-11T19:55:40.133+0000] {task_command.py:388} INFO - Running <TaskInstance: s3_to_snowflake.upload_or_snowflake manual__2023-03-11T19:50:13.559472+00:00 [running]> on host a2514c64f69a
[2023-03-11T19:55:40.195+0000] {taskinstance.py:1509} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=s3_to_snowflake
AIRFLOW_CTX_TASK_ID=upload_or_snowflake
AIRFLOW_CTX_EXECUTION_DATE=2023-03-11T19:50:13.559472+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-03-11T19:50:13.559472+00:00
[2023-03-11T19:55:40.196+0000] {s3_to_snowflake.py:146} INFO - Executing COPY command...
[2023-03-11T19:55:40.207+0000] {base.py:73} INFO - Using connection ID 'snowflake_default' for task execution.
[2023-03-11T19:55:40.209+0000] {connection.py:287} INFO - Snowflake Connector for Python Version: 3.0.1, Python Version: 3.7.16, Platform: Linux-5.15.49-linuxkit-aarch64-with-debian-11.6
[2023-03-11T19:55:40.211+0000] {connection.py:990} INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
[2023-03-11T19:55:40.211+0000] {connection.py:1007} INFO - Setting use_openssl_only mode to False
[2023-03-11T19:55:40.800+0000] {cursor.py:738} INFO - query: [ALTER SESSION SET autocommit=True]
[2023-03-11T19:55:40.882+0000] {cursor.py:751} INFO - query execution done
[2023-03-11T19:55:40.885+0000] {cursor.py:891} INFO - Number of results in first chunk: 1
[2023-03-11T19:55:40.886+0000] {sql.py:375} INFO - Running statement: COPY INTO dev.state_data
FROM @s3_stage/
files=('or_20230311.csv')
file_format=csvformat, parameters: None
[2023-03-11T19:55:40.886+0000] {cursor.py:738} INFO - query: [COPY INTO dev.state_data FROM @s3_stage/ files=('or_20230311.csv') file_format=c...]
[2023-03-11T19:55:41.367+0000] {cursor.py:751} INFO - query execution done
[2023-03-11T19:55:41.372+0000] {connection.py:586} INFO - closed
[2023-03-11T19:55:41.439+0000] {connection.py:589} INFO - No async queries seem to be running, deleting session
[2023-03-11T19:55:41.518+0000] {taskinstance.py:1768} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/snowflake/transfers/s3_to_snowflake.py", line 147, in execute
    snowflake_hook.run(copy_query, self.autocommit)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/snowflake/hooks/snowflake.py", line 379, in run
    self._run_command(cur, sql_statement, parameters)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/common/sql/hooks/sql.py", line 380, in _run_command
    cur.execute(sql_statement)
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/cursor.py", line 839, in execute
    Error.errorhandler_wrapper(self.connection, self, error_class, errvalue)
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/errors.py", line 294, in errorhandler_wrapper
    error_value,
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/errors.py", line 345, in hand_to_other_handler
    cursor.errorhandler(connection, cursor, error_class, error_value)
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/errors.py", line 231, in default_errorhandler
    cursor=cursor,
snowflake.connector.errors.ProgrammingError: 100080 (22000): 01aae18b-0000-4fb4-0004-9dfe000142fe: Number of columns in file (1) does not match that of the corresponding table (41), use file format option error_on_column_count_mismatch=false to ignore this error
  File 'or_20230311.csv', line 3, character 1
  Row 1 starts at line 2, column "STATE_DATA"["DATE":1]
  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. For more information on loading options, please run 'info loading_data' in a SQL client.
[2023-03-11T19:55:41.535+0000] {taskinstance.py:1323} INFO - Marking task as FAILED. dag_id=s3_to_snowflake, task_id=upload_or_snowflake, execution_date=20230311T195013, start_date=20230311T195539, end_date=20230311T195541
[2023-03-11T19:55:41.555+0000] {standard_task_runner.py:105} ERROR - Failed to execute job 94 for task upload_or_snowflake (100080 (22000): 01aae18b-0000-4fb4-0004-9dfe000142fe: Number of columns in file (1) does not match that of the corresponding table (41), use file format option error_on_column_count_mismatch=false to ignore this error
  File 'or_20230311.csv', line 3, character 1
  Row 1 starts at line 2, column "STATE_DATA"["DATE":1]
  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. For more information on loading options, please run 'info loading_data' in a SQL client.; 7494)
[2023-03-11T19:55:41.607+0000] {local_task_job.py:208} INFO - Task exited with return code 1
[2023-03-11T19:55:41.625+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
